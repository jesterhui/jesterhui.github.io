<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://jesterhui.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://jesterhui.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-01-10T07:55:17+00:00</updated><id>https://jesterhui.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Instruction fine-tuning Microsoft’s Phi-2 transformer</title><link href="https://jesterhui.github.io/blog/2024/sft-phi2/" rel="alternate" type="text/html" title="Instruction fine-tuning Microsoft’s Phi-2 transformer"/><published>2024-01-09T00:00:00+00:00</published><updated>2024-01-09T00:00:00+00:00</updated><id>https://jesterhui.github.io/blog/2024/sft-phi2</id><content type="html" xml:base="https://jesterhui.github.io/blog/2024/sft-phi2/"><![CDATA[<h2 id="another-blog">Another blog?</h2> <blockquote> <p>“Men will literally start a machine learning blog where they learn in public instead of going to therapy.”</p> </blockquote> <p>I love machine learning and think that the fact that the field has arrived in a big way is both awesome and deserved. I do find that there are some unforunate consequences that have arisen as a result of all of the money and attention, however:</p> <ol> <li> <p>The exact mechanics behind today’s biggest breakthroughs are shrouded in secrecy from the public. Although there is still lots of publishing happening in academia, the insights of the true luminaries in the field remain obscured from the general public behind the veil of the top foundation model labs.</p> </li> <li> <p>There is a deluge of newcomers to the field, which is totally awesome! But along with the new faces comes an unintended flood of low-quality information and tutorials. Personally, I’ve found it somewhat difficult to differentiate the signal from the noise. Additionlly, I’ve found that most tutorials use the Hugging Face trainer, which I can’t stand. I understand the value of it as an abstraction and making finetuning more accessible, but I much prefer the PyTorch training loop of <code class="language-plaintext highlighter-rouge">for batch in dataloader: optimizer.zero_grad(); loss = model(batch); loss.backward(); optimizer.step()</code>.</p> </li> </ol> <p>And now I’m adding my own blog to the zeitgeist. Probably no one will read it, but writing it has been helpful for me and maybe reading it can be helpful for you too. I want to apologize in advance because the level of complexity I use in explaining concepts might not be suitable or helpful for any audience really (i.e., too complex for beginners, banal for experts). But again, I’m mostly writing this for myself. Anyways, feel free to shoot my an email if you want to discuss anything. I’m always happy to chat about machine learning, and I’m always looking to learn more.</p> <h2 id="introduction">Introduction</h2> <p>Unsupervised pre-training (e.g., masked language modeling or next token prediction) followed by fine-tuning is the dominant paradigm in language modeling right now, and has been for several years. The high-level intuition behind why this approach works so well is that the unsupervised pretraining step allows models to learn the statistical patterns that are requisite for a general understanding of language. After the pretraining imbues general language skills on the model, it can then be fine-tuned to be performant on a target task using a smaller and task-specific dataset.</p> <p>Like so many other advances in the field of natural language processing (NLP), the pre-train then fine-tune paradigm was developed at OpenAI by <a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf">Radford et al.</a> circa 2018. Initially, they were primarily focused on fine-tuning models for traditional NLP tasks like semantic texutal similarity and textual entailment, but over time it has become evident that this paradigm extends to generative models as well with apporaches like instruction tuning and reinfocement learning from human feedback (RLHF) and reinforcement learning from AI feedback (RLAIF). This post will discuss instruction fine-tuning, but a future blog post will cover RLHF and RLAIF approaches.</p> <h2 id="why-instruction-tuning">Why instruction tuning?</h2> <p>Instruction tuning is a method in which a generative model is fine-tuned to follow instructions given by a user in the input prompt. Intruction tuning was pioneered at google research by <a href="https://openreview.net/forum?id=gEZrGCozdqR">Wei et al.</a> ~2 years ago. Although instruction tuning is a nasecent line of inquiry in the bigger picture of scientific progress, it is already an essentially canonical method in generative modeling, particularly because getting instruction tuning to work really well (via supervised fine-tuning and then RLHF) was one of the critical enablers of ChatGPT. Anyways, how do we do it? This blog will focus on instruction fine-tuning Microsoft’s Phi-2 transformer on a single A100 in Google Colab.</p> <h2 id="the-dataset-oig-small-chip2">The dataset: OIG-small-chip2</h2> <p>Building the instruction tuning dataset is arguably the most critical step in the process of instruction tuning. The pivotal nature of the dataset arises due to the fact that the instruction tuning dataset quality significantly impacts the performance of the fine-tuned model. Consequently, careful consideration is given to the design of the instructions and the selection of the human experts who generate the outputs. For example, the FLAN collection from <a href="https://arxiv.org/abs/2301.13688">Longpre et al.</a> provides a blueprint for building instruction tuning datasets. They show that using techniques like mixture weighting, including few-shot prompt templates, data augmentation via input inversion all lead to improved intstruction tuned models. Although there are some general scientific best practices, curating instruction tuning datasets is largely a mixture between art and science at present. In fact, it seems like lots of innovation in the space actually takes place in Discord channels and by hobbyists these days, which is a crazy departure from where the machine learning field was 5 years ago. To develop an intuition into what these instruction-tuned datasets look like, I would highly recommend browsing some of the instruction tuning datsets available on Hugging Face, such as <a href="https://huggingface.co/datasets/teknium/openhermes">OpenHermes</a>.</p> <p>Despite the importance of the dataset, I have but limited time and only one A100 with which to perform ablation studies. Therefore, I just used the existing OIG-small-chip2 dataset. The OIG-small-chip2 dataset contains python code examples, natural instruction examples, generic harmless instruction examples, instructions and responses with lists, follow-up questions, Wikipedia toxic adversarial questions, and the grade school math GSM8K dataset. For more infromation, see the dataset page <a href="https://github.com/LAION-AI/Open-Instruction-Generalist/tree/main/small_instruction_set">here</a>.</p> <h2 id="the-model-microsofts-phi-2-model">The model: Microsoft’s Phi-2 model</h2> <p>First off—kudos to Microsoft for permissively licensing Phi-2 with the MIT license. Phi-2 is a transformer model with 2.7 billion parameters, which is extremely small compared to the current state-of-the-art models (<a href="https://www.semianalysis.com/p/gpt-4-architecture-infrastructure">per semianalysis</a>, GPT-4 has ~1.8 trillion parameters). However, Microsoft was able to achieve impressive performance from this model considering both the model size and its training data size of only 30B tokens. The authors attribute this performance to the high quality of their proprietary data, which they describe as “textbook-quality data” (you can access their paper <a href="https://arxiv.org/abs/2309.05463">here</a>).</p> <h2 id="the-fine-tuning-see-colab">The fine-tuning: See <a href="https://colab.research.google.com/drive/1bCyz6nnkbKQJt6VH_xxG0SkAIL41oY8m?usp=sharing">Colab</a></h2> <p>See the linked Colab for a full walkthrough of how to fine-tune Phi in less than 200 lines of code. In an <em>ideal</em> world (where I had more discretionary GPU access or time) I would have fine-tuned the hyperparameters including the training time in epochs, weight decay, batch size, and learning rate using Bayesian optimization. Here, I employed a vibes-based training strategy where I just picked the reasonable-seeming values of 2 epochs, a weight decay of 1e-4, a learning rate of 1e-5, and an effective batch size of 64.</p> <p>I used Microsoft’s DeepSpeed package to allow me to use a larger batch size here. DeepSpeed is a deep learning optimization library for streamlining both language model training and inference. In this finetuning I specifically employed the ZeRO level 2 optimizer, in which optimizer and gradient states were offloaded to the CPU to reduce the GPU memory utilization, which critically allowed me to use a larger effective batch size.</p> <h2 id="results-the-fine-tuned-model">Results: The fine-tuned model</h2> <p>The model takes about 3 hours to instruction tune. Given more time, I would certainly run the model on some benchmark tasks to get an objective measure of its performance. But on based vibes-only analysis, I can tell you that it is certainly far worse than GPT-4, which is wholly expected given the parameter disparity. However, it can still give reasonable answers. For example, when asked to produce a grocery list for stir fry:</p> <p><code class="language-plaintext highlighter-rouge">User: Give me a grocery list for stir fry</code> <code class="language-plaintext highlighter-rouge">Assistant: Here is a grocery list for stir fry:</code> <code class="language-plaintext highlighter-rouge">1. Chicken breast</code> <code class="language-plaintext highlighter-rouge">2. Broccoli</code> <code class="language-plaintext highlighter-rouge">3. Carrots</code> <code class="language-plaintext highlighter-rouge">4. Bell peppers</code> <code class="language-plaintext highlighter-rouge">5. Onion</code> <code class="language-plaintext highlighter-rouge">6. Garlic</code> <code class="language-plaintext highlighter-rouge">7. Ginger</code> <code class="language-plaintext highlighter-rouge">8. Soy sauce</code> <code class="language-plaintext highlighter-rouge">9. Rice vinegar</code> <code class="language-plaintext highlighter-rouge">10. Cornstarch</code> <code class="language-plaintext highlighter-rouge">11. Sesame oil</code> <code class="language-plaintext highlighter-rouge">12. Green onions</code> <code class="language-plaintext highlighter-rouge">13. Rice noodles</code> <code class="language-plaintext highlighter-rouge">14. Peanuts</code> <code class="language-plaintext highlighter-rouge">15. Scallions</code> <code class="language-plaintext highlighter-rouge">16. Cilantro</code> <code class="language-plaintext highlighter-rouge">17. Sesame seeds</code></p> <p>which sounds like a good stir fry to me!</p> <h2 id="conclusion">Conclusion</h2> <p>Anyways, that’s a wrap! In future, if I had more GPUs, I would be interested in seeing how larger models do, like Mixtral 8x7B. I hope that I conveyed through this blog post that instruction fine-tuning requires both a deep understanding of data preparation and language model fine-tuning techniques. The result is an adaptive model that can field human-specified tasks and instructions, providing more accurate and useful responses. I expect that increasingly sophisticated variants of instruction fine-tuning (such as explanation tuning from <a href="https://arxiv.org/pdf/2306.02707.pdf">Mukherjee et al.</a>) will continue to play a crucial role in shaping language models in the coming years.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Another blog? “Men will literally start a machine learning blog where they learn in public instead of going to therapy.”]]></summary></entry><entry><title type="html">Notes on DPO</title><link href="https://jesterhui.github.io/blog/2023/dpo/" rel="alternate" type="text/html" title="Notes on DPO"/><published>2023-12-26T00:00:00+00:00</published><updated>2023-12-26T00:00:00+00:00</updated><id>https://jesterhui.github.io/blog/2023/dpo</id><content type="html" xml:base="https://jesterhui.github.io/blog/2023/dpo/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>I’ve recently been trying to learn about Reinforcement Learning from Human Feedback (RLHF), which is presently one of the leading methods for aligning language models to generate helpful answers that align with human preferences. Historically, reinforcement learning has focused on learning policies for really hard control problems such as Dota 2 or tokamak plasma control for nuclear fusion in magnetic confinement. Although there are many RL paradigms, most focus on learning policies that maximize the total reward (e.g., the score in pong). Similarly, RLHF aims to train a word generation policy that maximizes the reward, which in this case is given by human preference.</p> <p>Although the exact current reinforcement learning paradigms used by leading AI labs remain somewhat secretive, it’s known that Proximal Policy Optimization (PPO) was an initial choice at OpenAI. PPO, an actor-critic reinforcement learning algorithm, optimizes a policy in a way that improves a surrogate reward model while keeping the policy’s Kullback-Leibler divergence with the original (post supervised fine-tuning) policy low.</p> <p>However, a novel paradigm from Stanford University, presented at NeurIPS 2023, is Direct Preference Optimization (DPO). DPO is a model-free RLHF method that updates policy weights directly using preference data, thus bypassing the intermediary step of training a reward model. I thought taking some notes on DPO would help my own learning of DPO, and maybe sharing them could help some others learn more about the algorithm as well.</p> <h1 id="mathematical-foundations">Mathematical Foundations</h1> <h2 id="a-key-concepts-in-rl-and-rlhf">A. Key Concepts in RL and RLHF</h2> <p>I’ll start by laying out some of the basic foundations of RL, which, like many methods within machine learning, take their origins in the foundational work of Russian mathematician Andrey Markov. Specifically, Markov decision processes (MDPs) form the basis of many RL algorithms. MDPs provide a mathematical framework for modeling stochastic decision-making processes, and are characterized by states, actions, rewards, and transition probabilities.</p> <p>In the context of RL, each state represents a specific scenario faced by the RL agent. Using the example of pong from earlier, each state would correspond to a different set of pixel values, which in turn correspond to a different set of paddle and ball placements. Actions are the choices that the policy can make—such as moving the paddle up or down. Rewards are predefined based on a desired outcome for the learned policy, such as scoring points in Pong. RLHF modifies this paradigm by using human preference data to determine the reward structure.</p> <p>Determining the optimal policy (\(\pi^*\)) to take at a given state is a recursive problem that requires calculating the reward values given by the actions and state transitions available in future states as well. This gives rise to the Bellman equation, developed by Richard Bellman (who was briefly a professor at my alma mater, USC!). The Bellman equation mathematically formalizes the recursive relationship for calculating an agent’s optimal policy. More specifically, the Bellman equation states that the value of a state \(s\) under a particular policy \(\pi\) equals the immediate reward from that state plus the discounted future value of the state \(s'\) that the agent will land in next. Mathematically, it can be expressed as:</p> \[V_{\pi^*}(s) = \max_a \sum_{s'} T(s, a, s') [R(s, a, s') + \gamma V(s')]\] <p>Where \(V(s)\) is the total reward valuation of state \(s\) (assuming the optimal action \(a\) is taken), \(T(s, a, s')\) represents the transition probability from state \(s\) to state \(s'\) given that action \(a\) is taken, \(R(s, a, s')\) is the reward associated with action \(a\) after transitioning from \(s\) to \(s'\), and \(\gamma\) is the future discount factor, which is a fractional value between 0 and 1.</p> <h3 id="incorporating-human-feedback-into-mdps-and-bellman-equation">Incorporating Human Feedback into MDPs and Bellman Equation</h3> <p>In RLHF, human feedback dynamically influences the reward function, ( R(s, a, s’) ), in the Bellman Equation. This incorporation can be direct, where feedback adjusts the rewards of specific actions or states, or indirect, where it influences the model’s understanding of states or actions themselves. For instance, if human feedback indicates that a particular word choice or conversational path is preferred, the reward for actions leading to that path can be adjusted to reflect this preference.</p> <p>The challenge lies in quantifying this feedback accurately and integrating it into the MDP framework. This is where algorithms like DPO make a significant impact, as they can update policy weights directly based on preference data, circumventing some of the complexities involved in modifying the reward function directly.</p> <h2 id="b-math-behind-dpo">B. Math behind DPO</h2> <p>As mentioned previously, although the current RLHF paradigms used by leading AI labs remain somewhat secretive, it is known that PPO was an initial choice at OpenAI. PPO’s primary goal is to ensure that an updated policy does not stray too far from the original policy, making learning more stable and reliable. PPO is an actor-critic method, which means that the ‘actor’ (the LLM) proposes a set of actions, and the ‘critic’ (the reward model) evaluates them. The actor learns to optimize the policy based on the critic’s feedback, leading to more efficient and balanced policy updates. PPO also uses a Kullback-Leibler divergence penalty to ensure that the LLM’s generation policy remains similar to the original, post-supervised-fine-tuning policy, mitigating the risk of making too drastic a change based on new data.</p> <p>The policy loss function for PPO is given by:</p> \[\max_{\pi_\theta} \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_\theta} [ r_{\phi}(\theta)] - \beta \mathbb{D}_\text{KL}\left[\pi_{\theta}(y \vert x) \| \pi_{ref}(y \vert x)\right] ]\] <p>Where \(\theta\) represents the parameters of the policy, \(r_\phi(x, y)\) is a parameterized reward model, \(\mathbb{D}_\text{KL}\left[\pi_{\theta}(y \vert x) \| \pi_{ref}(y \vert x)\right]\) is the KL divergence between the new and reference policies that penalizes large policy changes, and \(\beta\) is a coefficient that controls the strength of the KL penalty.</p> <h3 id="formulating-dpo">Formulating DPO</h3> <h3 id="example-mathematical-model">Example: Mathematical model</h3> <h1 id="direct-preference-optimization-dpo-in-rlhf">Direct Preference Optimization (DPO) in RLHF</h1> <h2 id="a-understanding-dpo">A. Understanding DPO</h2> <h3 id="definition-and-application-in-rl">Definition and application in RL</h3> <h3 id="dpo-and-learning-efficiency">DPO and learning efficiency</h3> <h2 id="b-dpo-in-practice">B. DPO in Practice</h2> <h3 id="strategies-and-case-studies">Strategies and case studies</h3> <h3 id="code-example-dpo-implementation">Code example: DPO implementation</h3> <h1 id="practical-implementation-and-coding">Practical Implementation and Coding</h1> <h2 id="a-environment-setup-and-requirements">A. Environment Setup and Requirements</h2> <h2 id="b-coding-an-rlhf-model-with-dpo">B. Coding an RLHF Model with DPO</h2> <h3 id="step-by-step-guide">Step-by-step guide</h3> <h3 id="example-code">Example code</h3> <h1 id="conclusion">Conclusion</h1> <ul> <li> <p>Summary and future directions</p> </li> <li> <p>Encouragement for further exploration</p> </li> </ul> <h1 id="references">References</h1> <ul> <li>Curated list of essential readings</li> </ul>]]></content><author><name></name></author><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://jesterhui.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://jesterhui.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://jesterhui.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[]]></content><author><name></name></author></entry></feed>