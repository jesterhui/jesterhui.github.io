<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://jesterhui.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://jesterhui.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-08-15T05:41:24+00:00</updated><id>https://jesterhui.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Analogies between markets and fantasy football</title><link href="https://jesterhui.github.io/blog/2024/ff-markets/" rel="alternate" type="text/html" title="Analogies between markets and fantasy football"/><published>2024-08-14T00:00:00+00:00</published><updated>2024-08-14T00:00:00+00:00</updated><id>https://jesterhui.github.io/blog/2024/ff-markets</id><content type="html" xml:base="https://jesterhui.github.io/blog/2024/ff-markets/"><![CDATA[<p>Understanding team projections is a crucial piece of the puzzle for gaining a competitive edge in fantasy football. While discussions on individual talents often take center stage, sharp drafters know know that team-level projections are of near equal importance.</p> <p>Interestingly, the relationship between team projections and individual player projections is strikingly analogous to the concept of total addressable market (TAM) in business.</p> <ul> <li><strong>TAM Primacy</strong>: A company‚Äôs valuation is capped by its Total Addressable Market (TAM), just as a player‚Äôs fantasy ceiling is bound by their team‚Äôs offensive environment.</li> <li><strong>Market Cap Parallel</strong>: Transcendent market caps typically emerge from vast TAMs, mirroring how record-breaking fantasy seasons usually occur within high-powered offenses.</li> <li><strong>Small Market Constraints</strong>: Companies can achieve respectable valuations in small TAMs but won‚Äôt become the next Google. Similarly, talented players on low-volume offenses may be efficient but top fantasy charts at a lower rate.</li> <li><strong>Risk and Reward</strong>: Emerging markets offer the highest potential returns despite uncertainty. In fantasy, evolving offenses or shifting strategies present prime opportunities. While others seek ‚Äúsafe‚Äù picks, embracing calculated risk often yields league-winning advantages.</li> </ul> <p>Of course, there are exceptions to all of these rules. However, they help form a simple mental model for building an investment thesis on players, helping managers identify both hidden gems in expanding markets and potential busts in shrinking ones.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Understanding team projections is a crucial piece of the puzzle for gaining a competitive edge in fantasy football. While discussions on individual talents often take center stage, sharp drafters know know that team-level projections are of near equal importance.]]></summary></entry><entry><title type="html">Errors in Machine Learning Benchmark Datasets</title><link href="https://jesterhui.github.io/blog/2024/benchmarks/" rel="alternate" type="text/html" title="Errors in Machine Learning Benchmark Datasets"/><published>2024-01-31T00:00:00+00:00</published><updated>2024-01-31T00:00:00+00:00</updated><id>https://jesterhui.github.io/blog/2024/benchmarks</id><content type="html" xml:base="https://jesterhui.github.io/blog/2024/benchmarks/"><![CDATA[<h2 id="introduction-the-importance-of-dataset-quality">Introduction: The importance of dataset quality</h2> <p>It‚Äôs interesting that, while the importance of dataset quality is almost axiomatic in the physical sciences, machine learning research has overwhelmingly focused on model improvements rather than dataset improvements. Researchers primarily use performance on existing benchmark datasets as a proxy for model improvements. This is likely because creating a high-quality dataset is a much harder task than training a model. Whole companies, such as Scale AI, exist only to build high-fidelity machine learning datasets. In fact, many of these benchmarks, which are intended to be ‚Äúgold standard‚Äù datasets contain labeling errors (e.g., ImageNet, Amazon Reviews) as shown by <a href="https://arxiv.org/abs/2103.14749">Northcutt et al.</a> Importantly, if such issues can plague meticulously curated benchmarks, you can be sure that these types of label errors are even more pervasive in real-world datasets that are used to inform high-stakes decisions in the financial, legal, and healthcare domains.</p> <p>Understanding and correcting noisy labels in these datasets is key (üîê) to mitigating risk and improving decision making. In this blog, I explore the confident learning approach developed by <a href="https://www.jair.org/index.php/jair/article/view/12125">Northcutt et al.</a>, and apply it to the MultiNLI (Multi-Genre Natural Language Inference) dataset. This dataset forms the basis for one of the tasks in the canonical ‚ÄúGLUE‚Äù natural language processing benchmark.</p> <h2 id="textual-entailment-and-the-multinli-dataset">Textual entailment and the MultiNLI dataset</h2> <p>Textual entailment is a natural language processing task that involves understanding the logical relationship between two pieces of text called the ‚Äúpremise‚Äù and the ‚Äúhypothesis‚Äù. Entailment can be framed as a three-class classification task in which a model attempts to determine if the ‚Äúhypothesis‚Äù can be logically inferred from the ‚Äúpremise,‚Äù assigning the two pieces of text to one of three possible labels: contradiction, neutral, and entailment. Examples of each of these labels in the MultiNLI dataset are shown below:</p> <p><strong>Contradiction</strong></p> <p><em>Premise:</em> Your contribution helped make it possible for us to provide our students with a quality education.</p> <p><em>Hypothesis:</em> Your contributions were of no help with our students‚Äô education.</p> <p><strong>Neutral:</strong></p> <p><em>Premise:</em> yeah well you‚Äôre a student right</p> <p><em>Hypothesis:</em> Well you‚Äôre a mechanics student right?</p> <p><strong>Entailment:</strong></p> <p><em>Premise:</em> The other name, native well is, as a later explorer David Carnegie, author of Spinifex and Sand (1898), points out, a misnomer.</p> <p><em>Hypothesis:</em> The alternative name, resulting from a translation, was a misnomer according to the explorer David Carnegie.</p> <p>The MultiNLI dataset (which was created at NYU by <a href="https://aclanthology.org/N18-1101/">Williams et al.</a>) contains about 433k such sentence pairs annotated with textual entailment labels. The crowd-labeled pairs are taken from a variety of genres of both spoken and written text.</p> <h2 id="confident-learning-a-method-for-identifying-noisy-labels">Confident learning: a method for identifying noisy labels</h2> <p>Confident Learning is a data-centric approach for identifying which data in a dataset has noisy labels (i.e., which data is mislabeled or confusing). The approach was developed at MIT by <a href="https://arxiv.org/abs/1911.00068">Northcutt et al.</a>. The intuition behind this approach is that a model‚Äôs confidence in its predictions on a held-out set can be used to identify and correct mislabeled data within that held-out set (hence the name confident learning). From a practical standpoint, given a fixed classification ontology, data points identified as having noisy labels can either be relabeled or removed.</p> <p>Confident learning can also reveal when a classification ontology is not well-structured. Take, for instance, an image classification task that differentiates between two breeds of cows: Ayrshire and Guernsey.</p> <div class="which_cow"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/which_breed_of_dairy_cow_are_you-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/which_breed_of_dairy_cow_are_you-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/which_breed_of_dairy_cow_are_you-1400.webp"/> <img src="/assets/img/which_breed_of_dairy_cow_are_you.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Which breed of dairy cow are YOU? </div> <p>Due to the visual similarities between these breeds, classifying them based on images alone may result in low prediction confidence. Simply discarding these data is likely sub-optimal, however, as the classifier is accurately recognizing the subjects as cows. Here, the issue may lie in the design of the classification ontology itself, and it might be more practical to combine these two categories into a single, broader class, such as ‚Äòbrown cow‚Äô.</p> <p>Feel free to skip this next portion if you are not interested in the math, it‚Äôs not necessarily important for developing an intuition around the approach. But I like math so here is a little: Confident learning assumes that all datapoints in a labeled dataset, \(X := (x, \tilde{y})\) (where \(\tilde{y}\) is the potentially noisy assigned label), have a latent true label \(y^*\). Confident learning aims to estimate \(p(\tilde{y}, y^*)\), which is the joint distribution between the noisy and true labels. This is by creating a matrix called the confident joint, which is given by:</p> \[C_{\tilde{y}, y^*}[i][j] := |\hat{X}_{\tilde{y}=i,y^*=j}| \text{ where } \hat{X}_{\tilde{y}=i,y^*=j} := \{ x \in X_{\tilde{y}=i} : \hat{p}(y = j; x, \theta) \geq t_j \}\] <p>where the threshold \(t_j\) is given by</p> \[t_j = \frac{1}{|\hat{X}_{\tilde{y}=j}|} \sum_{x \in \hat{X}_{\tilde{y}=j}} \hat{p}(\tilde{y} = j; x, \theta)\] <p>Each entry in the confident joint is described by a count of the number of items in the dataset that are labeled as \(\tilde{y}\) and for which the model is confident that the true label is \(y^*\). Items that fall on the diagonal are items that are labeled correctly.</p> <h2 id="finetuning-a-bert-model-on-the-multinli-dataset"><a href="https://colab.research.google.com/drive/1ceVmv5bkLjCSmRT7Ios1MXhI9jJIaqWg?usp=sharing">Finetuning a BERT model on the MultiNLI dataset</a></h2> <p>I link a Colab notebook for running the model training, as well as performing confident learning to identify noisy labels. I performed hyperparameter tuning on the training time in epochs, weight decay, batch size, and learning rate using the <a href="https://docs.ray.io/en/latest/tune/index.html">Ray Tune</a> hyperparameter tuning package. By default, Ray Tune uses a tree-structured parzen estimator approach, which is a Bayesian optimization method introduced by <a href="https://papers.nips.cc/paper_files/paper/2011/hash/86e8f7ab32cfd12577bc2619bc635690-Abstract.html">Bergstra et al.</a>. I identified the optimal training hyperparameters to be training for 1 epoch with a batch size of 64, and a learning rate of 2.88e-05. I still performed training on an A100 to investigate the impact of larger batch sizes, but it is actually possible to train this model on a lower-in-memory GPU like a T4 or V100.</p> <h2 id="looking-for-label-errors">Looking for label errors</h2> <p>After training a model on the MultiNLI task, I used the confident learning approach to look for label errors in the matched validation set. The confident joint matrix for the matched validation set is shown below.</p> <div class="confident_joint"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/confident_joint-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/confident_joint-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/confident_joint-1400.webp"/> <img src="/assets/img/confident_joint.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Confident joint for the MultiNLI matched validation set. The x-axis corresponds to the latent true label, and the y-axis corresponds to the predicted label. </div> <p>Of the 9815 entries, 595 were off-diagonal in the confident joint, corresponding to ~6% of all entries. The data suggest that most label noise arises in relation to the neutral class. This is to be expected, as the neutral class bridges the other two classes and is generally the most nebulous of the three. The data suggest that there is some labeler bias away from the contradiction label towards the entailment label, with the entries below the diagonal in the confident joint having more entries than the entries above the diagonal.</p> <p>Below, I present a selection of cherry-picked off-diagonal entries. Upon a vibes-based manual inspection (extremely rigorous), these entries generally fall into three categories:</p> <ol> <li>There are indeed many mislabeled data points. This is somewhat expected given the crowd-sourced nature of the dataset, even though there were likely multiple annotators per data point.</li> <li>Many off-diagonal entries contain vague or unrelated premises or hypotheses. These are not only more difficult to label but also likely irrelevant to the task.</li> <li>Lastly, a significant number of the off-diagonal examples are simply hard! As a result, they are either frequently mislabeled by human annotators or confidently mislabeled by the model.</li> </ol> <p><strong>\(\tilde{y}=entailment, y^*=neutral\)</strong></p> <ul> <li><em>Premise:</em> ‚ÄúIn the meantime, the philosophy is to seize present-day opportunities in the thriving economy.‚Äù <em>Hypothesis:</em> ‚ÄúThe philosophy was to seize opportunities when the economy adding lots of jobs.‚Äù <em>Explanation of label:</em> The hypothesis does not necessarily follow from the premise. While the premise suggests a general philosophy of seizing opportunities in a thriving economy, the hypothesis narrows this down to a specific time when the economy is adding lots of jobs. This specificity introduces ambiguity, as the premise does not provide information about the economy adding jobs. Therefore, the label of ‚Äòentailment‚Äô is incorrect, and ‚Äòneutral‚Äô is more appropriate as the hypothesis is plausible but not guaranteed by the premise.</li> <li><em>Premise:</em> ‚ÄúIt was still night.‚Äù <em>Hypothesis:</em> ‚ÄúThe sun hadn‚Äôt risen yet, for the moon was shining daringly in the sky.‚Äù <em>Explanation of label:</em> The hypothesis can be a reasonable inference from the premise, as the presence of the moon in the sky typically indicates that it is still night and the sun has not risen. However, it is also possible for it to be night with no moon (or daytime with a moon), and so a neutral label may be more appropriate.</li> <li><em>Premise:</em> ‚ÄúAnyway, she was found dead this morning.‚Äù <em>Hypothesis:</em> ‚ÄúShe died during the night but wasn‚Äôt found until this morning.‚Äù <em>Explanation of label:</em> When she was found does not indicate when she died, although the most likely scenario (considering all possible deaths, which is a bit morose) is that she was found shortly after she died. A neutral label is more appropriate in this scenario.</li> </ul> <p><strong>\(\tilde{y}=entailment, y^*=contradiction\)</strong></p> <ul> <li><em>Premise:</em> ‚ÄúIt vibrated under his hand.‚Äù <em>Hypothesis:</em> ‚ÄúIt hummed quietly in his hand.‚Äù <em>Explanation of label:</em> Confident learning correctly identfies a mislabel, which arises due to a contradiction in the vibration‚Äôs location (i.e., under his hand vs. in his hand).</li> <li><em>Premise:</em> ‚ÄúThe Case Study Guidelines‚Äù <em>Hypothesis:</em> ‚ÄúGuidelines for the cast study.‚Äù <em>Explanation of label:</em> The label error here is due to a likely typo in the hypothesis, where ‚Äúcast study‚Äù should be ‚Äúcase study‚Äù. The labeler interpreted the typo, and assigned a entailment label. However, the typo does change the meaning such that the hypothesis does not follow from the premise.</li> <li><em>Premise:</em> ‚Äúdo you really romance‚Äù <em>Hypothesis:</em> ‚ÄúDo you really have an affair?‚Äù <em>Explanation of label:</em> The premise is not only a sentence fragment but also extremely vague, making it difficult to determine the relationship with the hypothesis.</li> </ul> <p><strong>\(\tilde{y}=neutral, y^*=entailment\)</strong></p> <ul> <li><em>Premise:</em> ‚ÄúSo, which one of you ladies wants to go first.‚Äù <em>Hypothesis:</em> ‚ÄúOne of the ladies should go first.‚Äù <em>Explanation of label:</em> A mislabeled point where the hypothesis is clearly a reasonable inference from the premise, as the speaker is clearly implying that one of the ladies should go first.</li> <li><em>Premise:</em> ‚Äúsee too much crime on TV and they think it‚Äôs way to go i don‚Äôt know what do you think‚Äù <em>Hypothesis:</em> ‚ÄúThey watch too much television.‚Äù <em>Explanation of label:</em> Here the hypothesis can be inferred from the premise, as the premise suggests that the subjects are influenced by the amount of crime they see on TV, which in turn implies they watch a lot of television. However, it is somewhat vague, as it is possible they were seeing too much crime on TV but not a large total volume of TV.</li> <li><em>Premise:</em> ‚ÄúWe also have found that leading organizations strive to ensure that their core processes efficiently and effectively support mission - related outcomes.‚Äù <em>Hypothesis:</em> ‚ÄúLeading organizations want to be sure their processes are successful.‚Äù <em>Explanation of label:</em> The hypothesis is a reasonable inference from the premise, as the premise states that leading organizations aim for their core processes to efficiently and effectively support mission-related outcomes, which can be interpreted as wanting their processes to be successful.</li> </ul> <p><strong>\(\tilde{y}=neutral, y^*=contradiction\)</strong></p> <ul> <li><em>Premise:</em> ‚Äúi don‚Äôt know if you have a place there called uh or you probably have something similar we call it Service Merchandise‚Äù <em>Hypothesis</em>: ‚ÄúYou probably have nothing like it.‚Äù <em>Explanation of label:</em> A clearly mislabeled point. The hypothesis directly contradicts the premise, which implies that a similar place exists, by saying ‚ÄúYou have nothing like it‚Äù.</li> <li><em>Premise:</em> ‚ÄúDoes anyone know what happened to chaos?‚Äù <em>Hypothesis</em>: ‚ÄúI know what happened to chaos.‚Äù <em>Explanation of label:</em> This is an interesting example, as the hypothesis seems to directly answer the question posed in the premise, which suggests that the speaker would know what happened to ‚Äòchaos‚Äô, which would indicate entailment. However, the premise could also be posed as a genuine question, which would then indicate that the hypothesis is a contradiction, as predicted by the model. Ultimately, it is a somewhat ill-posed problem in this case.</li> <li><em>Premise:</em> ‚Äúfacilitate suits for benefits by using the State and Federal courts and the independent bar on which those courts depend for the proper performance of their duties and responsibilities.‚Äù <em>Hypothesis:</em> ‚ÄúThe State and Federal courts are the same regardless of location.‚Äù <em>Explanation of label:</em> The premise emphasizes the fact that the State and Federal courts have independent bars. The hypothesis directly contradicts this by claiming the courts are uniform with respect to location.</li> </ul> <p><strong>\(\tilde{y}=contradiction, y^*=entailment\)</strong></p> <ul> <li><em>Premise:</em> ‚ÄúWithout the discount, nobody would buy the stock.‚Äù <em>Hypothesis:</em> ‚ÄúNobody would buy the stock if there was a discount.‚Äù <em>Explanation of label:</em> This is an interesing case because the given label is correct, and the predicted confident label is wrong. The hypothesis clearly contradicts the premise by suggesting that a discount would deter buyers, whereas the premise states that the absence of a discount would deter buyers.</li> <li><em>Premise:</em> ‚ÄúNHTSA concluded that while section 330 superseded the section 32902 criteria, it did not supersede the section 32902 mandate that there be CAFE standards for model year 1998.‚Äù <em>Hypothesis:</em> ‚ÄúNHTSA concluded that section 330 did not supersede the section 32902 mandate that there be CAFE standards for model year 1998.‚Äù <em>Explanation of label:</em> The hypothesis is a direct restatement of a part of the premise, specifically the conclusion drawn by NHTSA regarding the non-supersession of the section 32902 mandate by section 330. Therefore, the label of ‚Äòentailment‚Äô is appropriate as the hypothesis is explicitly supported by the premise.</li> <li><em>Premise:</em> ‚ÄúWe are concerned that the significant emissions reductions are required too quickly.‚Äù <em>Hypothesis:</em> ‚ÄúWe‚Äôre concerned about emissions reducing too quickly.‚Äù <em>Explanation of label:</em> The hypothesis is a paraphrase of the premise, expressing the same concern about the rapid pace of emissions reductions. Therefore, the label of ‚Äòcontradiction‚Äô is incorrect, and ‚Äòentailment‚Äô would be more appropriate as the hypothesis is essentially a rewording of the premise and does not introduce any new information or contradiction.</li> </ul> <p><strong>\(\tilde{y}=contradiction, y^*=neutral\)</strong></p> <ul> <li><em>Premise:</em> ‚ÄúMr. Erlenborn attended undergraduate courses at the University of Notre Dame, Indiana University, the University of Illinois, and Loyala University of Chicago.‚Äù <em>Hypothesis:</em> ‚ÄúMr. Erlenborn earned all of his undergraduate credits at the University of Notre Dame.‚Äù <em>Explanation of label:</em> The hypothesis suggests that Mr. Erlenborn completed his entire undergraduate education at the University of Notre Dame, which appears to contradict the premise that lists multiple universities where Mr. Erlenborn attended courses. However, neutral is more appropriate, as it may be possible that he attended but did not complete the classes at the other institutions.</li> <li><em>Premise:</em> ‚ÄúAt the same moment I felt a terrific blow on the back of my head She shuddered.‚Äù <em>Hypothesis:</em> ‚ÄúI was hit on by a nerdy guy at the local bar.‚Äù <em>Explanation of label:</em> The premise and the hypothesis are almost completely orthogonal here. Unless the nerdy guy is somehow adjacent to the blow on the back of the head, I can‚Äôt see how this is possibly a contradiction or an entailment. The neutral label seems most appropriate.</li> <li><em>Premise:</em> ‚ÄúAnother thing those early French and Dutch settlers agreed upon was that their island should be free of levies on any imported goods.‚Äù <em>Hypothesis:</em> ‚ÄúThe French settlers did not mind income taxes at all.‚Äù <em>Explanation of label:</em> The hypothesis states that the settlers were indifferent to income taxes, while the premies only discusses an agreement to avoid levies on imported goods. These are different kinds of taxes! Therefore, neutral would be the correct label.</li> </ul> <h2 id="final-thoughts">Final thoughts</h2> <p>I like confident learning! My assessment is that the primary benefits of confident learning are that it‚Äôs model agnostic and it works. I wonder if there is a way to incorporate uncertainty estimates in situations where they are available (neural network dropout, deep evidential classification). There is a whole company built around algorithmic datacleaning methods (with confident learning seeming to be one of the core offerings) now called <a href="https://cleanlab.ai/">Cleanlab</a>. Cool stuff!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Introduction: The importance of dataset quality]]></summary></entry><entry><title type="html">Instruction tuning Microsoft‚Äôs Phi-2 transformer</title><link href="https://jesterhui.github.io/blog/2024/sft-phi2/" rel="alternate" type="text/html" title="Instruction tuning Microsoft‚Äôs Phi-2 transformer"/><published>2024-01-09T00:00:00+00:00</published><updated>2024-01-09T00:00:00+00:00</updated><id>https://jesterhui.github.io/blog/2024/sft-phi2</id><content type="html" xml:base="https://jesterhui.github.io/blog/2024/sft-phi2/"><![CDATA[<h2 id="another-blog">Another blog?</h2> <blockquote> <p>‚ÄúMen will literally start a machine learning blog where they learn in public instead of going to therapy.‚Äù</p> </blockquote> <p>I love machine learning and think that the fact that the field has arrived in a big way is both awesome and deserved. I do find that there are some unforunate consequences that have arisen as a result of all of the money and attention, however:</p> <ol> <li> <p>The exact mechanics behind today‚Äôs biggest breakthroughs are shrouded in secrecy from the public. Although there is still lots of publishing happening in academia, the insights of the true luminaries in the field remain obscured from the general public behind the veil of the top foundation model labs.</p> </li> <li> <p>There is a deluge of newcomers to the field, which is totally awesome! But along with the new faces comes an unintended flood of low-quality information and tutorials. Personally, I‚Äôve found it somewhat difficult to differentiate the signal from the noise. Additionally, I‚Äôve found that most tutorials use the Hugging Face trainer, which I can‚Äôt stand. I understand the value of it as an abstraction and making finetuning more accessible, but I much prefer the PyTorch training loop of <code class="language-plaintext highlighter-rouge">for batch in dataloader: optimizer.zero_grad(); loss = model(batch); loss.backward(); optimizer.step()</code>.</p> </li> </ol> <p>And now I‚Äôm adding my own blog to the zeitgeist. Probably no one will read it, but writing it has been helpful for me and maybe reading it can be helpful for you too. I want to apologize in advance because the level of complexity I use in explaining concepts might not be suitable or helpful for any audience really (i.e., too complex for beginners, banal for experts). But again, I‚Äôm mostly writing this for myself. Anyways, feel free to shoot me an email if you want to discuss anything. I‚Äôm always happy to chat about machine learning, and I‚Äôm always looking to learn more.</p> <h2 id="introduction">Introduction</h2> <p>Unsupervised pre-training (e.g., masked language modeling or next token prediction) followed by fine-tuning is the dominant paradigm in language modeling right now, and has been for several years. The high-level intuition behind why this approach works so well is that the unsupervised pretraining step allows models to learn the statistical patterns that are requisite for a general understanding of language. After the pretraining imbues general language skills on the model, it can then be fine-tuned to be performant on a target task using a smaller and task-specific dataset.</p> <p>Like so many other advances in the field of natural language processing (NLP), the pre-train then fine-tune paradigm was developed at OpenAI by <a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf">Radford et al.</a> circa 2018. Initially, they were primarily focused on fine-tuning models for traditional NLP tasks like semantic texutal similarity and textual entailment, but over time it has become evident that this paradigm extends to generative models as well with apporaches like instruction tuning and reinforcement learning from human feedback (RLHF) and reinforcement learning from AI feedback (RLAIF). This post will discuss instruction fine-tuning, but a future blog post will cover RLHF and RLAIF approaches.</p> <h2 id="why-instruction-tuning">Why instruction tuning?</h2> <p>Instruction tuning is a method in which a generative model is fine-tuned to follow instructions given by a user in the input prompt. Intruction tuning was pioneered at google research by <a href="https://openreview.net/forum?id=gEZrGCozdqR">Wei et al.</a> ~2 years ago. Although instruction tuning is a nasecent line of inquiry in the bigger picture of scientific progress, it is already an essentially canonical method in generative modeling, particularly because getting instruction tuning to work really well (via supervised fine-tuning and then RLHF) was one of the critical enablers of ChatGPT. Anyways, how do we do it? This blog will focus on instruction fine-tuning Microsoft‚Äôs Phi-2 transformer on a single A100 in Google Colab.</p> <h2 id="the-dataset-oig-small-chip2">The dataset: OIG-small-chip2</h2> <p>Building the instruction tuning dataset is arguably the most critical step in the process of instruction tuning. The pivotal nature of the dataset arises due to the fact that the instruction tuning dataset quality significantly impacts the performance of the fine-tuned model. Consequently, careful consideration is given to the design of the instructions and the selection of the human experts who generate the outputs. For example, the FLAN collection from <a href="https://arxiv.org/abs/2301.13688">Longpre et al.</a> provides a blueprint for building instruction tuning datasets. They show that using techniques like mixture weighting, including few-shot prompt templates, data augmentation via input inversion all lead to improved intstruction tuned models. Although there are some general scientific best practices, curating instruction tuning datasets is largely a mixture between art and science at present. In fact, it seems like lots of innovation in the space actually takes place in Discord channels and by hobbyists these days, which is a crazy departure from where the machine learning field was 5 years ago. To develop an intuition into what these instruction-tuned datasets look like, I would highly recommend browsing some of the instruction tuning datsets available on Hugging Face, such as <a href="https://huggingface.co/datasets/teknium/openhermes">OpenHermes</a>.</p> <p>Despite the importance of the dataset, I have but limited time and only one A100 with which to perform ablation studies. Therefore, I just used the existing OIG-small-chip2 dataset. The OIG-small-chip2 dataset contains python code examples, natural instruction examples, generic harmless instruction examples, instructions and responses with lists, follow-up questions, Wikipedia toxic adversarial questions, and the grade school math GSM8K dataset. For more infromation, see the dataset page <a href="https://github.com/LAION-AI/Open-Instruction-Generalist/tree/main/small_instruction_set">here</a>.</p> <h2 id="the-model-microsofts-phi-2-model">The model: Microsoft‚Äôs Phi-2 model</h2> <p>First off‚Äîkudos to Microsoft for permissively licensing Phi-2 with the MIT license. Phi-2 is a transformer model with 2.7 billion parameters, which is extremely small compared to the current state-of-the-art models (<a href="https://www.semianalysis.com/p/gpt-4-architecture-infrastructure">per semianalysis</a>, GPT-4 has ~1.8 trillion parameters). However, Microsoft was able to achieve impressive performance from this model considering both the model size and its training data size of only 30B tokens. The authors attribute this performance to the high quality of their proprietary data, which they describe as ‚Äútextbook-quality data‚Äù (you can access their paper <a href="https://arxiv.org/abs/2309.05463">here</a>).</p> <h2 id="the-fine-tuning-see-colab">The fine-tuning: See <a href="https://colab.research.google.com/drive/1bCyz6nnkbKQJt6VH_xxG0SkAIL41oY8m?usp=sharing">Colab</a></h2> <p>See the linked Colab for a full walkthrough of how to fine-tune Phi in less than 200 lines of code. In an <em>ideal</em> world (where I had more discretionary GPU access or time) I would have fine-tuned the hyperparameters including the training time in epochs, weight decay, batch size, and learning rate using Bayesian optimization. Here, I employed a vibes-based training strategy where I just picked the reasonable-seeming values of 2 epochs, a weight decay of 1e-4, a learning rate of 1e-5, and an effective batch size of 64.</p> <p>I used Microsoft‚Äôs DeepSpeed package to allow me to use a larger batch size here. DeepSpeed is a deep learning optimization library for streamlining both language model training and inference. In this finetuning I specifically employed the ZeRO level 2 optimizer, in which optimizer and gradient states were offloaded to the CPU to reduce the GPU memory utilization, which critically allowed me to use a larger effective batch size.</p> <h2 id="results-the-fine-tuned-model">Results: The fine-tuned model</h2> <p>The model takes about 3 hours to instruction tune. Given more time, I would certainly run the model on some benchmark tasks to get an objective measure of its performance. But based on vibes-only analysis, I can tell you that it is certainly far worse than GPT-4, which is wholly expected given the parameter disparity. However, it can still give reasonable answers. For example, when asked to produce a grocery list for stir fry:</p> <p><code class="language-plaintext highlighter-rouge">User: Give me a grocery list for stir fry</code> <code class="language-plaintext highlighter-rouge">Assistant: Here is a grocery list for stir fry:</code> <code class="language-plaintext highlighter-rouge">1. Chicken breast</code> <code class="language-plaintext highlighter-rouge">2. Broccoli</code> <code class="language-plaintext highlighter-rouge">3. Carrots</code> <code class="language-plaintext highlighter-rouge">4. Bell peppers</code> <code class="language-plaintext highlighter-rouge">5. Onion</code> <code class="language-plaintext highlighter-rouge">6. Garlic</code> <code class="language-plaintext highlighter-rouge">7. Ginger</code> <code class="language-plaintext highlighter-rouge">8. Soy sauce</code> <code class="language-plaintext highlighter-rouge">9. Rice vinegar</code> <code class="language-plaintext highlighter-rouge">10. Cornstarch</code> <code class="language-plaintext highlighter-rouge">11. Sesame oil</code> <code class="language-plaintext highlighter-rouge">12. Green onions</code> <code class="language-plaintext highlighter-rouge">13. Rice noodles</code> <code class="language-plaintext highlighter-rouge">14. Peanuts</code> <code class="language-plaintext highlighter-rouge">15. Scallions</code> <code class="language-plaintext highlighter-rouge">16. Cilantro</code> <code class="language-plaintext highlighter-rouge">17. Sesame seeds</code></p> <p>which sounds like a good stir fry to me!</p> <h2 id="conclusion">Conclusion</h2> <p>Anyways, that‚Äôs a wrap! In future, if I had more GPUs, I would be interested in seeing how larger models do, like Mixtral 8x7B. I hope that I conveyed through this blog post that instruction fine-tuning requires both a deep understanding of data preparation and language model fine-tuning techniques. The result is an adaptive model that can field human-specified tasks and instructions, providing more accurate and useful responses. I expect that increasingly sophisticated variants of instruction fine-tuning (such as explanation tuning from <a href="https://arxiv.org/pdf/2306.02707.pdf">Mukherjee et al.</a>) will continue to play a crucial role in shaping language models in the coming years.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Another blog? ‚ÄúMen will literally start a machine learning blog where they learn in public instead of going to therapy.‚Äù]]></summary></entry></feed>