<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Errors in Machine Learning Benchmark Datasets | Jacques Esterhuizen</title> <meta name="author" content="Jacques Esterhuizen"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="machine learning, chemical engineering, catalysis, natural language processing, large language models, agents, scientist, generative ai, llms, genai"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jesterhui.github.io/blog/2024/benchmarks/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "Errors in Machine Learning Benchmark Datasets",
      "description": "",
      "published": "January 31, 2024",
      "authors": [
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Jacques¬†</span>Esterhuizen</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Errors in Machine Learning Benchmark Datasets</h1> <p></p> </d-title><d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#introduction-the-importance-of-dataset-quality">Introduction: The importance of dataset quality</a></div> <div><a href="#textual-entailment-and-the-multinli-dataset">Textual entailment and the MultiNLI dataset</a></div> <div><a href="#confident-learning-a-method-for-identifying-noisy-labels">Confident learning: A method for identifying noisy labels</a></div> <div><a href="#fine-tuning-a-bert-model-on-the-multinli-dataset">Fine-tuning a BERT model on the MultiNLI dataset</a></div> <div><a href="#looking-for-label-errors">Looking for label errors</a></div> </nav> </d-contents> <h2 id="introduction-the-importance-of-dataset-quality">Introduction: The importance of dataset quality</h2> <p>It‚Äôs interesting that, while the importance of dataset quality is almost axiomatic in the physical sciences, machine learning research has overwhelmingly focused on model improvements rather than dataset improvements. Researchers primarily use performance on existing benchmark datasets as a proxy for model improvements. This is likely because creating a high-quality dataset is a much harder task than training a model. Whole companies, such as Scale AI, exist only to build high-fidelity machine learning datasets. In fact, many of these benchmarks, which are intended to be ‚Äúgold standard‚Äù datasets contain labeling errors (e.g., ImageNet, Amazon Reviews) as shown by <a href="https://arxiv.org/abs/2103.14749" rel="external nofollow noopener" target="_blank">Northcutt et al.</a> Importantly, if such issues can plague meticulously curated benchmarks, you can be sure that these types of label errors are even more pervasive in real-world datasets that are used to inform high-stakes decisions in the financial, legal, and healthcare domains.</p> <p>Understanding and correcting noisy labels in these datasets is key (üîê) to mitigating risk and improving decision making. In this blog, I explore the confident learning approach developed by <a href="https://www.jair.org/index.php/jair/article/view/12125" rel="external nofollow noopener" target="_blank">Northcutt et al.</a>, and apply it to the MultiNLI (Multi-Genre Natural Language Inference) dataset. This dataset forms the basis for one of the tasks in the canonical ‚ÄúGLUE‚Äù natural language processing benchmark.</p> <h2 id="textual-entailment-and-the-multinli-dataset">Textual entailment and the MultiNLI dataset</h2> <p>Textual entailment is a natural language processing task that involves understanding the logical relationship between two pieces of text called the ‚Äúpremise‚Äù and the ‚Äúhypothesis‚Äù. Entailment can be framed as a three-class classification task in which a model attempts to determine if the ‚Äúhypothesis‚Äù can be logically inferred from the ‚Äúpremise,‚Äù assigning the two pieces of text to one of three possible labels: contradiction, neutral, and entailment. Examples of each of these labels in the MultiNLI dataset are shown below:</p> <p><strong>Contradiction</strong></p> <p><em>Premise:</em> Your contribution helped make it possible for us to provide our students with a quality education.</p> <p><em>Hypothesis:</em> Your contributions were of no help with our students‚Äô education.</p> <p><strong>Neutral:</strong></p> <p><em>Premise:</em> yeah well you‚Äôre a student right</p> <p><em>Hypothesis:</em> Well you‚Äôre a mechanics student right?</p> <p><strong>Entailment:</strong></p> <p><em>Premise:</em> The other name, native well is, as a later explorer David Carnegie, author of Spinifex and Sand (1898), points out, a misnomer.</p> <p><em>Hypothesis:</em> The alternative name, resulting from a translation, was a misnomer according to the explorer David Carnegie.</p> <p>The MultiNLI dataset (which was created at NYU by <a href="https://aclanthology.org/N18-1101/" rel="external nofollow noopener" target="_blank">Williams et al.</a>) contains about 433k such sentence pairs annotated with textual entailment labels. The crowd-labeled pairs are taken from a variety of genres of both spoken and written text.</p> <h2 id="confident-learning-a-method-for-identifying-noisy-labels">Confident learning: a method for identifying noisy labels</h2> <p>Confident Learning is a data-centric approach for identifying which data in a dataset has noisy labels (i.e., which data is mislabeled or confusing). The approach was developed at MIT by <a href="https://arxiv.org/abs/1911.00068" rel="external nofollow noopener" target="_blank">Northcutt et al.</a>. The intuition behind this approach is that a model‚Äôs confidence in its predictions on a held-out set can be used to identify and correct mislabeled data within that held-out set (hence the name confident learning). From a practical standpoint, given a fixed classification ontology, data points identified as having noisy labels can either be relabeled or removed.</p> <p>Confident learning can also reveal when a classification ontology is not well-structured. Take, for instance, an image classification task that differentiates between two breeds of cows: Ayrshire and Guernsey.</p> <div class="which_cow"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/which_breed_of_dairy_cow_are_you-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/which_breed_of_dairy_cow_are_you-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/which_breed_of_dairy_cow_are_you-1400.webp"></source> <img src="/assets/img/which_breed_of_dairy_cow_are_you.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> Which breed of dairy cow are YOU? </div> <p>Due to the visual similarities between these breeds, classifying them based on images alone may result in low prediction confidence. Simply discarding these data is likely sub-optimal, however, as the classifier is accurately recognizing the subjects as cows. Here, the issue may lie in the design of the classification ontology itself, and it might be more practical to combine these two categories into a single, broader class, such as ‚Äòbrown cow‚Äô.</p> <p>Feel free to skip this next portion if you are not interested in the math, it‚Äôs not necessarily important for developing an intuition around the approach. But I like math so here is a little: Confident learning assumes that all datapoints in a labeled dataset, \(X := (x, \tilde{y})\) (where \(\tilde{y}\) is the potentially noisy assigned label), have a latent true label \(y^*\). Confident learning aims to estimate \(p(\tilde{y}, y^*)\), which is the joint distribution between the noisy and true labels. This is by creating a matrix called the confident joint, which is given by:</p> \[C_{\tilde{y}, y^*}[i][j] := |\hat{X}_{\tilde{y}=i,y^*=j}| \text{ where } \hat{X}_{\tilde{y}=i,y^*=j} := \{ x \in X_{\tilde{y}=i} : \hat{p}(y = j; x, \theta) \geq t_j \}\] <p>where the threshold \(t_j\) is given by</p> \[t_j = \frac{1}{|\hat{X}_{\tilde{y}=j}|} \sum_{x \in \hat{X}_{\tilde{y}=j}} \hat{p}(\tilde{y} = j; x, \theta)\] <p>Each entry in the confident joint is described by a count of the number of items in the dataset that are labeled as \(\tilde{y}\) and for which the model is confident that the true label is \(y^*\). Items that fall on the diagonal are items that are labeled correctly.</p> <h2 id="finetuning-a-bert-model-on-the-multinli-dataset"><a href="https://colab.research.google.com/drive/1ceVmv5bkLjCSmRT7Ios1MXhI9jJIaqWg?usp=sharing" rel="external nofollow noopener" target="_blank">Finetuning a BERT model on the MultiNLI dataset</a></h2> <p>I link a Colab notebook for running the model training, as well as performing confident learning to identify noisy labels. I performed hyperparameter tuning on the training time in epochs, weight decay, batch size, and learning rate using the <a href="https://docs.ray.io/en/latest/tune/index.html" rel="external nofollow noopener" target="_blank">Ray Tune</a> hyperparameter tuning package. By default, Ray Tune uses a tree-structured parzen estimator approach, which is a Bayesian optimization method introduced by <a href="https://papers.nips.cc/paper_files/paper/2011/hash/86e8f7ab32cfd12577bc2619bc635690-Abstract.html" rel="external nofollow noopener" target="_blank">Bergstra et al.</a>. I identified the optimal training hyperparameters to be training for 1 epoch with a batch size of 64, and a learning rate of 2.88e-05. I still performed training on an A100 to investigate the impact of larger batch sizes, but it is actually possible to train this model on a lower-in-memory GPU like a T4 or V100.</p> <h2 id="looking-for-label-errors">Looking for label errors</h2> <p>After training a model on the MultiNLI task, I used the confident learning approach to look for label errors in the matched validation set. The confident joint matrix for the matched validation set is shown below.</p> <div class="confident_joint"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/confident_joint-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/confident_joint-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/confident_joint-1400.webp"></source> <img src="/assets/img/confident_joint.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> Confident joint for the MultiNLI matched validation set. The x-axis corresponds to the latent true label, and the y-axis corresponds to the predicted label. </div> <p>Of the 9815 entries, 595 were off-diagonal in the confident joint, corresponding to ~6% of all entries. The data suggest that most label noise arises in relation to the neutral class. This is to be expected, as the neutral class bridges the other two classes and is generally the most nebulous of the three. The data suggest that there is some labeler bias away from the contradiction label towards the entailment label, with the entries below the diagonal in the confident joint having more entries than the entries above the diagonal.</p> <p>Below, I present a selection of cherry-picked off-diagonal entries. Upon a vibes-based manual inspection (extremely rigorous), these entries generally fall into three categories:</p> <ol> <li>There are indeed many mislabeled data points. This is somewhat expected given the crowd-sourced nature of the dataset, even though there were likely multiple annotators per data point.</li> <li>Many off-diagonal entries contain vague or unrelated premises or hypotheses. These are not only more difficult to label but also likely irrelevant to the task.</li> <li>Lastly, a significant number of the off-diagonal examples are simply hard! As a result, they are either frequently mislabeled by human annotators or confidently mislabeled by the model.</li> </ol> <p><strong>\(\tilde{y}=entailment, y^*=neutral\)</strong></p> <ul> <li> <em>Premise:</em> ‚ÄúIn the meantime, the philosophy is to seize present-day opportunities in the thriving economy.‚Äù <em>Hypothesis:</em> ‚ÄúThe philosophy was to seize opportunities when the economy adding lots of jobs.‚Äù <em>Explanation of label:</em> The hypothesis does not necessarily follow from the premise. While the premise suggests a general philosophy of seizing opportunities in a thriving economy, the hypothesis narrows this down to a specific time when the economy is adding lots of jobs. This specificity introduces ambiguity, as the premise does not provide information about the economy adding jobs. Therefore, the label of ‚Äòentailment‚Äô is incorrect, and ‚Äòneutral‚Äô is more appropriate as the hypothesis is plausible but not guaranteed by the premise.</li> <li> <em>Premise:</em> ‚ÄúIt was still night.‚Äù <em>Hypothesis:</em> ‚ÄúThe sun hadn‚Äôt risen yet, for the moon was shining daringly in the sky.‚Äù <em>Explanation of label:</em> The hypothesis can be a reasonable inference from the premise, as the presence of the moon in the sky typically indicates that it is still night and the sun has not risen. However, it is also possible for it to be night with no moon (or daytime with a moon), and so a neutral label may be more appropriate.</li> <li> <em>Premise:</em> ‚ÄúAnyway, she was found dead this morning.‚Äù <em>Hypothesis:</em> ‚ÄúShe died during the night but wasn‚Äôt found until this morning.‚Äù <em>Explanation of label:</em> When she was found does not indicate when she died, although the most likely scenario (considering all possible deaths, which is a bit morose) is that she was found shortly after she died. A neutral label is more appropriate in this scenario.</li> </ul> <p><strong>\(\tilde{y}=entailment, y^*=contradiction\)</strong></p> <ul> <li> <em>Premise:</em> ‚ÄúIt vibrated under his hand.‚Äù <em>Hypothesis:</em> ‚ÄúIt hummed quietly in his hand.‚Äù <em>Explanation of label:</em> Confident learning correctly identfies a mislabel, which arises due to a contradiction in the vibration‚Äôs location (i.e., under his hand vs. in his hand).</li> <li> <em>Premise:</em> ‚ÄúThe Case Study Guidelines‚Äù <em>Hypothesis:</em> ‚ÄúGuidelines for the cast study.‚Äù <em>Explanation of label:</em> The label error here is due to a likely typo in the hypothesis, where ‚Äúcast study‚Äù should be ‚Äúcase study‚Äù. The labeler interpreted the typo, and assigned a entailment label. However, the typo does change the meaning such that the hypothesis does not follow from the premise.</li> <li> <em>Premise:</em> ‚Äúdo you really romance‚Äù <em>Hypothesis:</em> ‚ÄúDo you really have an affair?‚Äù <em>Explanation of label:</em> The premise is not only a sentence fragment but also extremely vague, making it difficult to determine the relationship with the hypothesis.</li> </ul> <p><strong>\(\tilde{y}=neutral, y^*=entailment\)</strong></p> <ul> <li> <em>Premise:</em> ‚ÄúSo, which one of you ladies wants to go first.‚Äù <em>Hypothesis:</em> ‚ÄúOne of the ladies should go first.‚Äù <em>Explanation of label:</em> A mislabeled point where the hypothesis is clearly a reasonable inference from the premise, as the speaker is clearly implying that one of the ladies should go first.</li> <li> <em>Premise:</em> ‚Äúsee too much crime on TV and they think it‚Äôs way to go i don‚Äôt know what do you think‚Äù <em>Hypothesis:</em> ‚ÄúThey watch too much television.‚Äù <em>Explanation of label:</em> Here the hypothesis can be inferred from the premise, as the premise suggests that the subjects are influenced by the amount of crime they see on TV, which in turn implies they watch a lot of television. However, it is somewhat vague, as it is possible they were seeing too much crime on TV but not a large total volume of TV.</li> <li> <em>Premise:</em> ‚ÄúWe also have found that leading organizations strive to ensure that their core processes efficiently and effectively support mission - related outcomes.‚Äù <em>Hypothesis:</em> ‚ÄúLeading organizations want to be sure their processes are successful.‚Äù <em>Explanation of label:</em> The hypothesis is a reasonable inference from the premise, as the premise states that leading organizations aim for their core processes to efficiently and effectively support mission-related outcomes, which can be interpreted as wanting their processes to be successful.</li> </ul> <p><strong>\(\tilde{y}=neutral, y^*=contradiction\)</strong></p> <ul> <li> <em>Premise:</em> ‚Äúi don‚Äôt know if you have a place there called uh or you probably have something similar we call it Service Merchandise‚Äù <em>Hypothesis</em>: ‚ÄúYou probably have nothing like it.‚Äù <em>Explanation of label:</em> A clearly mislabeled point. The hypothesis directly contradicts the premise, which implies that a similar place exists, by saying ‚ÄúYou have nothing like it‚Äù.</li> <li> <em>Premise:</em> ‚ÄúDoes anyone know what happened to chaos?‚Äù <em>Hypothesis</em>: ‚ÄúI know what happened to chaos.‚Äù <em>Explanation of label:</em> This is an interesting example, as the hypothesis seems to directly answer the question posed in the premise, which suggests that the speaker would know what happened to ‚Äòchaos‚Äô, which would indicate entailment. However, the premise could also be posed as a genuine question, which would then indicate that the hypothesis is a contradiction, as predicted by the model. Ultimately, it is a somewhat ill-posed problem in this case.</li> <li> <em>Premise:</em> ‚Äúfacilitate suits for benefits by using the State and Federal courts and the independent bar on which those courts depend for the proper performance of their duties and responsibilities.‚Äù <em>Hypothesis:</em> ‚ÄúThe State and Federal courts are the same regardless of location.‚Äù <em>Explanation of label:</em> The premise emphasizes the fact that the State and Federal courts have independent bars. The hypothesis directly contradicts this by claiming the courts are uniform with respect to location.</li> </ul> <p><strong>\(\tilde{y}=contradiction, y^*=entailment\)</strong></p> <ul> <li> <em>Premise:</em> ‚ÄúWithout the discount, nobody would buy the stock.‚Äù <em>Hypothesis:</em> ‚ÄúNobody would buy the stock if there was a discount.‚Äù <em>Explanation of label:</em> This is an interesing case because the given label is correct, and the predicted confident label is wrong. The hypothesis clearly contradicts the premise by suggesting that a discount would deter buyers, whereas the premise states that the absence of a discount would deter buyers.</li> <li> <em>Premise:</em> ‚ÄúNHTSA concluded that while section 330 superseded the section 32902 criteria, it did not supersede the section 32902 mandate that there be CAFE standards for model year 1998.‚Äù <em>Hypothesis:</em> ‚ÄúNHTSA concluded that section 330 did not supersede the section 32902 mandate that there be CAFE standards for model year 1998.‚Äù <em>Explanation of label:</em> The hypothesis is a direct restatement of a part of the premise, specifically the conclusion drawn by NHTSA regarding the non-supersession of the section 32902 mandate by section 330. Therefore, the label of ‚Äòentailment‚Äô is appropriate as the hypothesis is explicitly supported by the premise.</li> <li> <em>Premise:</em> ‚ÄúWe are concerned that the significant emissions reductions are required too quickly.‚Äù <em>Hypothesis:</em> ‚ÄúWe‚Äôre concerned about emissions reducing too quickly.‚Äù <em>Explanation of label:</em> The hypothesis is a paraphrase of the premise, expressing the same concern about the rapid pace of emissions reductions. Therefore, the label of ‚Äòcontradiction‚Äô is incorrect, and ‚Äòentailment‚Äô would be more appropriate as the hypothesis is essentially a rewording of the premise and does not introduce any new information or contradiction.</li> </ul> <p><strong>\(\tilde{y}=contradiction, y^*=neutral\)</strong></p> <ul> <li> <em>Premise:</em> ‚ÄúMr. Erlenborn attended undergraduate courses at the University of Notre Dame, Indiana University, the University of Illinois, and Loyala University of Chicago.‚Äù <em>Hypothesis:</em> ‚ÄúMr. Erlenborn earned all of his undergraduate credits at the University of Notre Dame.‚Äù <em>Explanation of label:</em> The premise provides specific opening hours for the museum, which include weekdays and Saturday mornings, but makes no mention of Sunday. The hypothesis states that the museum is not open on Sunday, which is a reasonable inference based on the absence of Sunday in the listed opening hours. Therefore, the label of ‚Äòcontradiction‚Äô is incorrect, and ‚Äòentailment‚Äô would be more appropriate as the hypothesis is supported by the premise.</li> <li> <em>Premise:</em> ‚ÄúAt the same moment I felt a terrific blow on the back of my head She shuddered.‚Äù <em>Hypothesis:</em> ‚ÄúI was hit on by a nerdy guy at the local bar.‚Äù <em>Explanation of label:</em> The premise and the hypothesis are almost completely orthogonal here. Unless the nerdy guy is somehow adjacent to the blow on the back of the head, I can‚Äôt see how this is possibly a contradiction or an entailment. The neutral label seems most appropriate.</li> <li> <em>Premise:</em> ‚ÄúAnother thing those early French and Dutch settlers agreed upon was that their island should be free of levies on any imported goods.‚Äù <em>Hypothesis:</em> ‚ÄúThe French settlers did not mind income taxes at all.‚Äù <em>Explanation of label:</em> The hypothesis states that the settlers were indifferent to income taxes, while the premies only discusses an agreement to avoid levies on imported goods. These are different kinds of taxes! Therefore, neutral would be the correct label.</li> </ul> <h2 id="final-thoughts">Final thoughts</h2> <p>I like confident learning! My assessment is that the primary benefits of confident learning are that it‚Äôs model agnostic and it works. I wonder if there is a way to incorporate uncertainty estimates in situations where they are available (neural network dropout, deep evidential classification). There is a whole company built around algorithmic datacleaning methods (with confident learning seeming to be one of the core offerings) now called <a href="https://cleanlab.ai/" rel="external nofollow noopener" target="_blank">Cleanlab</a>. Cool stuff!</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> </div> <footer class="fixed-bottom"> <div class="container mt-0"> ¬© Copyright 2024 Jacques Esterhuizen. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>