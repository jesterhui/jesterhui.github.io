<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <link rel="icon" type="image/png" href="/favicon.png"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Notes on DPO | Jacques Esterhuizen</title> <meta name="author" content="Jacques Esterhuizen"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="machine learning, chemical engineering, catalysis, natural language processing, large language models, agents, scientist, generative ai, llms, genai"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jesterhui.github.io/blog/2023/dpo/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Jacques </span>Esterhuizen</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Notes on DPO</h1> <p class="post-meta">December 26, 2023</p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="introduction">Introduction</h1> <p>I’ve recently been trying to learn about Reinforcement Learning from Human Feedback (RLHF), which is presently one of the leading methods for aligning language models to generate helpful answers that align with human preferences. Historically, reinforcement learning has focused on learning policies for really hard control problems such as Dota 2 or tokamak plasma control for nuclear fusion in magnetic confinement. Although there are many RL paradigms, most focus on learning policies that maximize the total reward (e.g., the score in pong). Similarly, RLHF aims to train a word generation policy that maximizes the reward, which in this case is given by human preference.</p> <p>Although the exact current reinforcement learning paradigms used by leading AI labs remain somewhat secretive, it’s known that Proximal Policy Optimization (PPO) was an initial choice at OpenAI. PPO, an actor-critic reinforcement learning algorithm, optimizes a policy in a way that improves a surrogate reward model while keeping the policy’s Kullback-Leibler divergence with the original (post supervised fine-tuning) policy low.</p> <p>However, a novel paradigm from Stanford University, presented at NeurIPS 2023, is Direct Preference Optimization (DPO). DPO is a model-free RLHF method that updates policy weights directly using preference data, thus bypassing the intermediary step of training a reward model. I thought taking some notes on DPO would help my own learning of DPO, and maybe sharing them could help some others learn more about the algorithm as well.</p> <h1 id="mathematical-foundations">Mathematical Foundations</h1> <h2 id="a-key-concepts-in-rl-and-rlhf">A. Key Concepts in RL and RLHF</h2> <p>I’ll start by laying out some of the basic foundations of RL, which, like many methods within machine learning, take their origins in the foundational work of Russian mathematician Andrey Markov. Specifically, Markov decision processes (MDPs) form the basis of many RL algorithms. MDPs provide a mathematical framework for modeling stochastic decision-making processes, and are characterized by states, actions, rewards, and transition probabilities.</p> <p>In the context of RL, each state represents a specific scenario faced by the RL agent. Using the example of pong from earlier, each state would correspond to a different set of pixel values, which in turn correspond to a different set of paddle and ball placements. Actions are the choices that the policy can make—such as moving the paddle up or down. Rewards are predefined based on a desired outcome for the learned policy, such as scoring points in Pong. RLHF modifies this paradigm by using human preference data to determine the reward structure.</p> <p>Determining the optimal policy (\(\pi^*\)) to take at a given state is a recursive problem that requires calculating the reward values given by the actions and state transitions available in future states as well. This gives rise to the Bellman equation, developed by Richard Bellman (who was briefly a professor at my alma mater, USC!). The Bellman equation mathematically formalizes the recursive relationship for calculating an agent’s optimal policy. More specifically, the Bellman equation states that the value of a state \(s\) under a particular policy \(\pi\) equals the immediate reward from that state plus the discounted future value of the state \(s'\) that the agent will land in next. Mathematically, it can be expressed as:</p> \[V_{\pi^*}(s) = \max_a \sum_{s'} T(s, a, s') [R(s, a, s') + \gamma V(s')]\] <p>Where \(V(s)\) is the total reward valuation of state \(s\) (assuming the optimal action \(a\) is taken), \(T(s, a, s')\) represents the transition probability from state \(s\) to state \(s'\) given that action \(a\) is taken, \(R(s, a, s')\) is the reward associated with action \(a\) after transitioning from \(s\) to \(s'\), and \(\gamma\) is the future discount factor, which is a fractional value between 0 and 1.</p> <h3 id="incorporating-human-feedback-into-mdps-and-bellman-equation">Incorporating Human Feedback into MDPs and Bellman Equation</h3> <p>In RLHF, human feedback dynamically influences the reward function, ( R(s, a, s’) ), in the Bellman Equation. This incorporation can be direct, where feedback adjusts the rewards of specific actions or states, or indirect, where it influences the model’s understanding of states or actions themselves. For instance, if human feedback indicates that a particular word choice or conversational path is preferred, the reward for actions leading to that path can be adjusted to reflect this preference.</p> <p>The challenge lies in quantifying this feedback accurately and integrating it into the MDP framework. This is where algorithms like DPO make a significant impact, as they can update policy weights directly based on preference data, circumventing some of the complexities involved in modifying the reward function directly.</p> <h2 id="b-math-behind-dpo">B. Math behind DPO</h2> <p>As mentioned previously, although the current RLHF paradigms used by leading AI labs remain somewhat secretive, it is known that PPO was an initial choice at OpenAI. PPO’s primary goal is to ensure that an updated policy does not stray too far from the original policy, making learning more stable and reliable. PPO is an actor-critic method, which means that the ‘actor’ (the LLM) proposes a set of actions, and the ‘critic’ (the reward model) evaluates them. The actor learns to optimize the policy based on the critic’s feedback, leading to more efficient and balanced policy updates. PPO also uses a Kullback-Leibler divergence penalty to ensure that the LLM’s generation policy remains similar to the original, post-supervised-fine-tuning policy, mitigating the risk of making too drastic a change based on new data.</p> <p>The policy loss function for PPO is given by:</p> \[\max_{\pi_\theta} \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_\theta} [ r_{\phi}(\theta)] - \beta \mathbb{D}_\text{KL}\left[\pi_{\theta}(y \vert x) \| \pi_{ref}(y \vert x)\right] ]\] <p>Where \(\theta\) represents the parameters of the policy, \(r_\phi(x, y)\) is a parameterized reward model, \(\mathbb{D}_\text{KL}\left[\pi_{\theta}(y \vert x) \| \pi_{ref}(y \vert x)\right]\) is the KL divergence between the new and reference policies that penalizes large policy changes, and \(\beta\) is a coefficient that controls the strength of the KL penalty.</p> <h3 id="formulating-dpo">Formulating DPO</h3> <h3 id="example-mathematical-model">Example: Mathematical model</h3> <h1 id="direct-preference-optimization-dpo-in-rlhf">Direct Preference Optimization (DPO) in RLHF</h1> <h2 id="a-understanding-dpo">A. Understanding DPO</h2> <h3 id="definition-and-application-in-rl">Definition and application in RL</h3> <h3 id="dpo-and-learning-efficiency">DPO and learning efficiency</h3> <h2 id="b-dpo-in-practice">B. DPO in Practice</h2> <h3 id="strategies-and-case-studies">Strategies and case studies</h3> <h3 id="code-example-dpo-implementation">Code example: DPO implementation</h3> <h1 id="practical-implementation-and-coding">Practical Implementation and Coding</h1> <h2 id="a-environment-setup-and-requirements">A. Environment Setup and Requirements</h2> <h2 id="b-coding-an-rlhf-model-with-dpo">B. Coding an RLHF Model with DPO</h2> <h3 id="step-by-step-guide">Step-by-step guide</h3> <h3 id="example-code">Example code</h3> <h1 id="conclusion">Conclusion</h1> <ul> <li> <p>Summary and future directions</p> </li> <li> <p>Encouragement for further exploration</p> </li> </ul> <h1 id="references">References</h1> <ul> <li>Curated list of essential readings</li> </ul> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/sft-phi2/">Instruction fine-tuning Microsoft's Phi-2 transformer</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/displaying-external-posts-on-your-al-folio-blog/">Displaying External Posts on Your al-folio Blog</a> </li> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Jacques Esterhuizen. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>