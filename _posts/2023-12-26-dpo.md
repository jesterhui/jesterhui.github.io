---
layout: distill
title: Notes on DPO
date: 2024-01-09
toc:
  - name: Another blog?
  - name: Introduction
  - name: Why instruction tuning?
  - name: "The dataset: OIG-small-chip2"
  - name: "The model: Microsoft's Phi-2 model"
  - name: "The fine-tuning: See Colab"
  - name: "Results: The fine-tuned model"
---
# Introduction

I've recently been trying to learn about Reinforcement Learning from Human Feedback (RLHF), which is presently one of the leading methods for aligning language models to generate helpful answers that align with human preferences. Historically, reinforcement learning has focused on learning policies for really hard control problems such as Dota 2 or tokamak plasma control for nuclear fusion in magnetic confinement. Although there are many RL paradigms, most focus on learning policies that maximize the total reward (e.g., the score in pong). Similarly, RLHF aims to train a word generation policy that maximizes the reward, which in this case is given by human preference.

Although the exact current reinforcement learning paradigms used by leading AI labs remain somewhat secretive, it's known that Proximal Policy Optimization (PPO) was an initial choice at OpenAI. PPO, an actor-critic reinforcement learning algorithm, optimizes a policy in a way that improves a surrogate reward model while keeping the policy's Kullback-Leibler divergence with the original (post supervised fine-tuning) policy low.

However, a novel paradigm from Stanford University, presented at NeurIPS 2023, is Direct Preference Optimization (DPO). DPO is a model-free RLHF method that updates policy weights directly using preference data, thus bypassing the intermediary step of training a reward model. I thought taking some notes on DPO would help my own learning of DPO, and maybe sharing them could help some others learn more about the algorithm as well.

# Mathematical Foundations

## A. Key Concepts in RL and RLHF

I'll start by laying out some of the basic foundations of RL, which, like many methods within machine learning, take their origins in the foundational work of Russian mathematician Andrey Markov. Specifically, Markov decision processes (MDPs) form the basis of many RL algorithms. MDPs provide a mathematical framework for modeling stochastic decision-making processes, and are characterized by states, actions, rewards, and transition probabilities.

In the context of RL, each state represents a specific scenario faced by the RL agent. Using the example of pong from earlier, each state would correspond to a different set of pixel values, which in turn correspond to a different set of paddle and ball placements. Actions are the choices that the policy can makeâ€”such as moving the paddle up or down. Rewards are predefined based on a desired outcome for the learned policy, such as scoring points in Pong. RLHF modifies this paradigm by using human preference data to determine the reward structure.

Determining the optimal policy ($$\pi^*$$) to take at a given state is a recursive problem that requires calculating the reward values given by the actions and state transitions available in future states as well. This gives rise to the Bellman equation, developed by Richard Bellman (who was briefly a professor at my alma mater, USC!). The Bellman equation mathematically formalizes the recursive relationship for calculating an agent's optimal policy. More specifically, the Bellman equation states that the value of a state $$s$$ under a particular policy $$\pi$$ equals the immediate reward from that state plus the discounted future value of the state $$s'$$ that the agent will land in next. Mathematically, it can be expressed as:

$$ V_{\pi^*}(s) = \max_a \sum_{s'} T(s, a, s') [R(s, a, s') + \gamma V(s')] $$

Where $$V(s)$$ is the total reward valuation of state $$s$$ (assuming the optimal action $$a$$ is taken), $$T(s, a, s')$$ represents the transition probability from state $$s$$ to state $$s'$$ given that action $$a$$ is taken, $$R(s, a, s')$$ is the reward associated with action $$a$$ after transitioning from $$s$$ to $$s'$$, and $$\gamma$$ is the future discount factor, which is a fractional value between 0 and 1.

### Incorporating Human Feedback into MDPs and Bellman Equation

In RLHF, human feedback dynamically influences the reward function, \( R(s, a, s') \), in the Bellman Equation. This incorporation can be direct, where feedback adjusts the rewards of specific actions or states, or indirect, where it influences the model's understanding of states or actions themselves. For instance, if human feedback indicates that a particular word choice or conversational path is preferred, the reward for actions leading to that path can be adjusted to reflect this preference.

The challenge lies in quantifying this feedback accurately and integrating it into the MDP framework. This is where algorithms like DPO make a significant impact, as they can update policy weights directly based on preference data, circumventing some of the complexities involved in modifying the reward function directly.

## B. Math behind DPO

As mentioned previously, although the current RLHF paradigms used by leading AI labs remain somewhat secretive, it is known that PPO was an initial choice at OpenAI. PPO's primary goal is to ensure that an updated policy does not stray too far from the original policy, making learning more stable and reliable. PPO is an actor-critic method, which means that the 'actor' (the LLM) proposes a set of actions, and the 'critic' (the reward model) evaluates them. The actor learns to optimize the policy based on the critic's feedback, leading to more efficient and balanced policy updates. PPO also uses a Kullback-Leibler divergence penalty to ensure that the LLM's generation policy remains similar to the original, post-supervised-fine-tuning policy, mitigating the risk of making too drastic a change based on new data.

The policy loss function for PPO is given by:

$$\max_{\pi_\theta} \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_\theta} [ r_{\phi}(\theta)] - \beta \mathbb{D}_\text{KL}\left[\pi_{\theta}(y \vert x) \| \pi_{ref}(y \vert x)\right] ]$$

Where $$\theta$$ represents the parameters of the policy, $$r_\phi(x, y)$$ is a parameterized reward model, $$\mathbb{D}_\text{KL}\left[\pi_{\theta}(y \vert x) \| \pi_{ref}(y \vert x)\right]$$ is the KL divergence between the new and reference policies that penalizes large policy changes, and $$\beta$$ is a coefficient that controls the strength of the KL penalty.

### Formulating DPO

### Example: Mathematical model

# Direct Preference Optimization (DPO) in RLHF

## A. Understanding DPO

### Definition and application in RL

### DPO and learning efficiency

## B. DPO in Practice

### Strategies and case studies

### Code example: DPO implementation

# Practical Implementation and Coding

## A. Environment Setup and Requirements

## B. Coding an RLHF Model with DPO

### Step-by-step guide

### Example code

# Conclusion

- Summary and future directions

- Encouragement for further exploration

# References

- Curated list of essential readings
